from flask import Flask, request, jsonify
import cv2
from PIL import Image
import io
import numpy as np
import base64
from flask_cors import CORS
import time
from skimage.metrics import structural_similarity as ssim
import torch
import torch.nn as nn
import os

app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": ["http://localhost:5173", "https://lewisMVP.github.io"]}})

def image_to_base64(img):
    # Kh√¥ng chuy·ªÉn ƒë·ªïi n·∫øu ·∫£nh ƒë√£ l√† RGB ho·∫∑c ·∫£nh grayscale
    if img is None:
        # Tr·∫£ v·ªÅ ·∫£nh tr·ªëng n·∫øu img l√† None
        img = np.zeros((100, 100, 3), dtype=np.uint8)
    
    _, buffer = cv2.imencode('.png', img)
    return base64.b64encode(buffer).decode('utf-8')

# Part A: Image Filtering
@app.route('/filter', methods=['POST'])
def filter_image():
    # Ki·ªÉm tra input
    if 'image' not in request.files:
        return jsonify({'error': 'No image provided'}), 400
    
    file = request.files['image']
    img = np.array(Image.open(file))
    
    # Chuy·ªÉn ƒë·ªïi ·∫£nh sang ƒë·ªãnh d·∫°ng ph√π h·ª£p (BGR)
    if len(img.shape) == 3 and img.shape[2] == 4:
        img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)
    elif len(img.shape) == 3:
        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    
    # L∆∞u l·∫°i ·∫£nh m√†u g·ªëc n·∫øu c√≥
    if len(img.shape) == 3:
        img_color = img.copy()
        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        img_gray = img.copy()
        img_color = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
    
    noisy_img = img_gray  # S·ª≠ d·ª•ng ·∫£nh g·ªëc thay v√¨ t·∫°o ·∫£nh nhi·ªÖu
    
    # √Åp d·ª•ng c√°c b·ªô l·ªçc l√†m n√©t theo y√™u c·∫ßu
    filters = {}
    psnr_values = {}
    ssim_values = {}
    computation_times = {}
    
    # 1. Mean filter
    start_time = time.time()
    filters['mean'] = cv2.blur(noisy_img, (5, 5))
    computation_times['mean'] = time.time() - start_time
    
    # 2. Gaussian filter
    start_time = time.time()
    filters['gaussian'] = cv2.GaussianBlur(noisy_img, (5, 5), 0)
    computation_times['gaussian'] = time.time() - start_time
    
    # 3. Median filter
    start_time = time.time()
    filters['median'] = cv2.medianBlur(noisy_img, 5)
    computation_times['median'] = time.time() - start_time
    
    # 4. Laplacian sharpening (n√¢ng cao bi√™n c·∫°nh)
    start_time = time.time()
    laplacian = cv2.Laplacian(noisy_img, cv2.CV_64F)
    laplacian = cv2.normalize(laplacian, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
    filters['laplacian'] = cv2.addWeighted(noisy_img, 1.0, laplacian, 0.5, 0)
    computation_times['laplacian'] = time.time() - start_time
    
    # T√≠nh PSNR v√† SSIM ƒë·ªÉ so s√°nh hi·ªáu qu·∫£ gi·∫£m nhi·ªÖu c·ªßa t·ª´ng b·ªô l·ªçc
    for key, filtered in filters.items():
        # T√≠nh PSNR
        mse = np.mean((img_gray - filtered) ** 2)
        if mse == 0:
            psnr_values[key] = 100
        else:
            max_pixel = 255.0
            psnr_values[key] = 20 * np.log10(max_pixel / np.sqrt(mse))
        
        # T√≠nh SSIM
        try:
            # H√†m compareSSIM trong cv2 ƒë∆∞·ª£c ƒë·ªïi t√™n th√†nh compare_ssim trong skimage.metrics
            from skimage.metrics import structural_similarity as ssim
            ssim_values[key] = ssim(img_gray, filtered, data_range=255)
        except ImportError:
            # N·∫øu kh√¥ng c√≥ skimage, th·ª≠ d√πng cv2 (phi√™n b·∫£n c≈©)
            try:
                ssim_values[key] = cv2.compareSSIM(img_gray, filtered)
            except:
                ssim_values[key] = -1  # kh√¥ng t√≠nh ƒë∆∞·ª£c SSIM
    
    # T√≠nh bi√™n c·∫°nh b·∫±ng Canny ƒë·ªÉ so s√°nh kh·∫£ nƒÉng b·∫£o to√†n bi√™n c·ªßa t·ª´ng b·ªô l·ªçc
    edge_preservation = {}
    edges_original = cv2.Canny(img_gray, 50, 150)
    for key, filtered in filters.items():
        edges_filtered = cv2.Canny(filtered, 50, 150)
        # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa hai ·∫£nh bi√™n c·∫°nh (ƒë·ªÉ ƒëo l∆∞·ªùng vi·ªác gi·ªØ bi√™n c·∫°nh)
        edge_preservation[key] = cv2.compareHist(
            cv2.calcHist([edges_original], [0], None, [256], [0, 256]), 
            cv2.calcHist([edges_filtered], [0], None, [256], [0, 256]),
            cv2.HISTCMP_CORREL
        )
    
    # Chuy·ªÉn c√°c ·∫£nh grayscale sang BGR ƒë·ªÉ hi·ªÉn th·ªã
    img_display_gray = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2BGR)  # ƒê·ªïi t√™n ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
    filters_display = {key: cv2.cvtColor(img, cv2.COLOR_GRAY2BGR) for key, img in filters.items()}
    
    # N·∫øu ·∫£nh ƒë·∫ßu v√†o l√† ·∫£nh m√†u, gi·ªØ nguy√™n m√†u g·ªëc, ng∆∞·ª£c l·∫°i chuy·ªÉn sang RGB
    if len(img.shape) == 3:
        img_display = img
    else:
        img_display = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)
    
    return jsonify({
        'original': image_to_base64(img_display),
        'grayscale': image_to_base64(img_display_gray),  # Tr·∫£ v·ªÅ ·∫£nh grayscale thay v√¨ ·∫£nh nhi·ªÖu
        'filtered': {key: image_to_base64(img) for key, img in filters_display.items()},
        'psnr': psnr_values,
        'ssim': ssim_values,
        'computation_time': computation_times,
        'edge_preservation': edge_preservation
    })

# Part B: 3D Reconstruction
@app.route('/3dconstruction', methods=['POST'])
def reconstruct_3d():
    # Ki·ªÉm tra input
    if 'left_image' not in request.files or 'right_image' not in request.files:
        return jsonify({'error': 'Both left and right images are required'}), 400
    
    left_file = request.files['left_image']
    right_file = request.files['right_image']
    num_disparities = int(request.form.get('num_disparities', 64))
    method = request.form.get('method', 'StereoBM')

    # ƒê·ªçc ·∫£nh m√†u
    left_img_color = cv2.imdecode(np.frombuffer(left_file.read(), np.uint8), cv2.IMREAD_COLOR)
    right_img_color = cv2.imdecode(np.frombuffer(right_file.read(), np.uint8), cv2.IMREAD_COLOR)
    
    if left_img_color is None or right_img_color is None:
        return jsonify({'error': 'Failed to read images. Ensure they are valid image files (JPEG, PNG, or PGM)'}), 400
    
    # Chuy·ªÉn ƒë·ªïi sang grayscale CH·ªà cho vi·ªác t√≠nh to√°n stereo matching
    left_gray = cv2.cvtColor(left_img_color, cv2.COLOR_BGR2GRAY)
    right_gray = cv2.cvtColor(right_img_color, cv2.COLOR_BGR2GRAY)
    
    # Ki·ªÉm tra k√≠ch th∆∞·ªõc ·∫£nh
    if left_gray.shape != right_gray.shape:
        return jsonify({'error': 'Left and right images must have the same dimensions'}), 400

    # Ki·ªÉm tra k√≠ch th∆∞·ªõc h·ª£p l·ªá
    h, w = left_gray.shape
    if h <= 0 or w <= 0:
        return jsonify({'error': 'Invalid image dimensions'}), 400
    
    # L∆∞u b·∫£n sao ·∫£nh m√†u g·ªëc ƒë·ªÉ hi·ªÉn th·ªã
    left_img_bgr = left_img_color.copy()
    right_img_bgr = right_img_color.copy()
    
    # Hi·ªáu ch·ªânh ·∫£nh (rectification) - Gi·∫£ ƒë·ªãnh th√¥ng s·ªë camera ƒë∆°n gi·∫£n
    focal_length = 600.0
    baseline = 80.0
    
    # Fix type mismatch in stereoRectify
    try:
        camera_matrix = np.array([[focal_length, 0, w/2],
                                [0, focal_length, h/2],
                                [0, 0, 1]], dtype=np.float64)
        dist_coeffs = np.zeros(5, dtype=np.float64)
        R = np.eye(3, dtype=np.float64)
        T = np.array([baseline, 0, 0], dtype=np.float64)
        
        R1, R2, P1, P2, Q, _, _ = cv2.stereoRectify(
            camera_matrix, dist_coeffs, camera_matrix, dist_coeffs,
            (w, h), R, T, alpha=0
        )
        
        # T·∫°o b·∫£n ƒë·ªì √°nh x·∫° ƒë·ªÉ hi·ªáu ch·ªânh ·∫£nh
        map1x, map1y = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, R1, P1, (w, h), cv2.CV_32FC1)
        map2x, map2y = cv2.initUndistortRectifyMap(camera_matrix, dist_coeffs, R2, P2, (w, h), cv2.CV_32FC1)
        
        # Hi·ªáu ch·ªânh ·∫£nh grayscale cho vi·ªác t√≠nh to√°n
        left_rectified = cv2.remap(left_gray, map1x, map1y, cv2.INTER_LINEAR)
        right_rectified = cv2.remap(right_gray, map2x, map2y, cv2.INTER_LINEAR)
        
        # Hi·ªáu ch·ªânh ·∫£nh m√†u cho vi·ªác hi·ªÉn th·ªã
        left_color_rectified = cv2.remap(left_img_color, map1x, map1y, cv2.INTER_LINEAR)
        right_color_rectified = cv2.remap(right_img_color, map2x, map2y, cv2.INTER_LINEAR)
        
        # S·ª≠ d·ª•ng ·∫£nh m√†u ƒë√£ hi·ªáu ch·ªânh cho hi·ªÉn th·ªã
        left_img_bgr = left_color_rectified
        right_img_bgr = right_color_rectified
    except cv2.error as e:
        print(f"Stereo rectification failed: {str(e)}")
        left_rectified = left_gray
        right_rectified = right_gray
        Q = np.float32([[1, 0, 0, -0.5 * w],
                      [0, -1, 0, 0.5 * h],
                      [0, 0, 0, focal_length],
                      [0, 0, -1/baseline, 0]])
    
    # T√≠nh b·∫£n ƒë·ªì disparity s·ª≠ d·ª•ng ·∫£nh grayscale
    if method == 'StereoSGBM':
        stereo = cv2.StereoSGBM_create(
            minDisparity=-16,
            numDisparities=num_disparities,
            blockSize=3,
            P1=8 * 3 * 3 ** 2,
            P2=32 * 3 * 3 ** 2,
            disp12MaxDiff=1,
            uniquenessRatio=10,
            speckleWindowSize=150,
            speckleRange=64
        )
    else:
        stereo = cv2.StereoBM_create(numDisparities=num_disparities, blockSize=5)
    
    # T√≠nh disparity map
    disparity = stereo.compute(left_rectified, right_rectified)
    disparity = cv2.filterSpeckles(disparity, 0, 4000, num_disparities)[0]
    print(f"Disparity min: {disparity.min()}, max: {disparity.max()}, mean: {disparity.mean()}")
    
    # T·∫°o disparity map chu·∫©n h√≥a
    disp_norm = cv2.normalize(disparity, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)
    
    # T·∫°o disparity map c√≥ m√†u (√°p d·ª•ng colormap)
    disp_color = cv2.applyColorMap(disp_norm, cv2.COLORMAP_JET)
    
    # T√°i t·∫°o ƒëi·ªÉm 3D
    points_3d = cv2.reprojectImageTo3D(disparity, Q)
    
    # L·∫•y th√¥ng tin m√†u t·ª´ ·∫£nh tr√°i ƒë√£ hi·ªáu ch·ªânh
    h, w = left_rectified.shape
    colors = left_color_rectified.reshape(-1, 3)  # Chuy·ªÉn th√†nh m·∫£ng c√°c ƒëi·ªÉm m√†u
    
    # Reshape points_3d ƒë·ªÉ x·ª≠ l√Ω
    points_3d_flat = points_3d.reshape(-1, 3)
    
    # Filter out invalid points
    valid_indices = np.isfinite(points_3d_flat).all(axis=1)
    points_3d_valid = points_3d_flat[valid_indices]
    colors_valid = colors[valid_indices]
    
    # Use percentile-based filtering to preserve structure
    z_vals = points_3d_valid[:, 2]
    x_vals = points_3d_valid[:, 0]
    y_vals = points_3d_valid[:, 1]

    # Remove extreme outliers (1st and 99th percentiles)
    z_min, z_max = np.percentile(z_vals, [1, 99])
    x_min, x_max = np.percentile(x_vals, [1, 99])
    y_min, y_max = np.percentile(y_vals, [1, 99])

    # Apply filters
    mask = (z_vals > z_min) & (z_vals < z_max)
    mask &= (x_vals > x_min) & (x_vals < x_max)
    mask &= (y_vals > y_min) & (y_vals < y_max)
    points_3d_filtered = points_3d_valid[mask]
    colors_filtered = colors_valid[mask]
    
    # Apply better scale factor depending on the range of values
    z_range = np.max(np.abs(points_3d_filtered[:, 2]))
    scale_factor = 10.0 / (z_range + 1e-6)  # Adaptive scaling
    points_3d_filtered = points_3d_filtered * scale_factor
    
    # Ensure points are within reasonable bounds for visualization
    max_distance = 20.0
    dist_mask = np.sum(points_3d_filtered**2, axis=1) < max_distance**2
    points_3d_filtered = points_3d_filtered[dist_mask]
    colors_filtered = colors_filtered[dist_mask]
    
    # Downsample if necessary to maintain performance
    if len(points_3d_filtered) > 10000:
        indices = np.random.choice(len(points_3d_filtered), 10000, replace=False)
        points_3d_filtered = points_3d_filtered[indices]
        colors_filtered = colors_filtered[indices]
    
    # Convert to list for JSON serialization
    points_with_colors = []
    for i in range(len(points_3d_filtered)):
        # Each point is [x, y, z, r, g, b]
        point_with_color = points_3d_filtered[i].tolist() + colors_filtered[i].tolist()
        points_with_colors.append(point_with_color)
    
    print(f"Final colored point cloud size: {len(points_with_colors)} points")

    # T√≠nh ma tr·∫≠n c∆° b·∫£n v√† ƒë∆∞·ªùng epipolar
    orb = cv2.ORB_create()
    kp1, des1 = orb.detectAndCompute(left_gray, None)
    kp2, des2 = orb.detectAndCompute(right_gray, None)
    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    matches = bf.match(des1, des2)
    matches = sorted(matches, key=lambda x: x.distance)[:10]
    pts1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
    pts2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
    F, _ = cv2.findFundamentalMat(pts1, pts2, cv2.FM_RANSAC)
    
    # V·∫Ω ƒë∆∞·ªùng epipolar tr√™n ·∫£nh tr√°i
    lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2, F)
    lines1 = lines1.reshape(-1, 3)
    left_epipolar = left_img_bgr.copy()
    for line, pt in zip(lines1, pts1):
        color = tuple(np.random.randint(0, 255, 3).tolist())
        x0, y0 = map(int, [0, -line[2] / line[1]])
        x1, y1 = map(int, [left_img_bgr.shape[1], -(line[2] + line[0] * left_img_bgr.shape[1]) / line[1]])
        cv2.line(left_epipolar, (x0, y0), (x1, y1), color, 1)
        cv2.circle(left_epipolar, tuple(map(int, pt[0])), 5, color, -1)
    
    # V·∫Ω ƒë∆∞·ªùng epipolar tr√™n ·∫£nh ph·∫£i
    lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)
    lines2 = lines2.reshape(-1, 3)
    right_epipolar = right_img_bgr.copy()
    for line, pt in zip(lines2, pts2):
        color = tuple(np.random.randint(0, 255, 3).tolist())
        x0, y0 = map(int, [0, -line[2] / line[1]])
        x1, y1 = map(int, [right_img_bgr.shape[1], -(line[2] + line[0] * right_img_bgr.shape[1]) / line[1]])
        cv2.line(right_epipolar, (x0, y0), (x1, y1), color, 1)
        cv2.circle(right_epipolar, tuple(map(int, pt[0])), 5, color, -1)
    
    return jsonify({
        'disparity': image_to_base64(disp_color),
        'points_3d': points_with_colors,
        'left_epipolar': image_to_base64(left_epipolar),
        'right_epipolar': image_to_base64(right_epipolar)
    })

# Part C: Image Stitching
def stitch_two_images(img1, img2):
    # ƒê·∫£m b·∫£o ·∫£nh c√πng ki·ªÉu d·ªØ li·ªáu v√† k√≠ch th∆∞·ªõc ph√π h·ª£p
    img1 = cv2.resize(img1, (0, 0), fx=1.0, fy=1.0)
    img2 = cv2.resize(img2, (0, 0), fx=1.0, fy=1.0)
    
    # Chuy·ªÉn ·∫£nh sang grayscale cho vi·ªác t√¨m ƒë·∫∑c tr∆∞ng
    gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
    
    # S·ª≠ d·ª•ng SIFT thay v√¨ ORB ƒë·ªÉ c√≥ k·∫øt qu·∫£ t·ªët h∆°n
    # N·∫øu SIFT kh√¥ng c√≥ s·∫µn, s·ª≠ d·ª•ng ORB v·ªõi s·ªë l∆∞·ª£ng features cao h∆°n
    try:
        sift = cv2.SIFT_create()
        kp1, des1 = sift.detectAndCompute(gray1, None)
        kp2, des2 = sift.detectAndCompute(gray2, None)
        
        # FLANN matcher ho·∫°t ƒë·ªông t·ªët h∆°n v·ªõi SIFT
        FLANN_INDEX_KDTREE = 1
        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
        search_params = dict(checks=50)
        flann = cv2.FlannBasedMatcher(index_params, search_params)
        matches = flann.knnMatch(des1, des2, k=2)
        
        # L·ªçc matches t·ªët b·∫±ng Lowe's ratio test
        good_matches = []
        for m, n in matches:
            if m.distance < 0.7 * n.distance:
                good_matches.append(m)
    except:
        # Fallback sang ORB n·∫øu SIFT kh√¥ng c√≥ s·∫µn
        orb = cv2.ORB_create(nfeatures=3000)
        kp1, des1 = orb.detectAndCompute(gray1, None)
        kp2, des2 = orb.detectAndCompute(gray2, None)
        
        # Brute force matcher cho ORB
        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        good_matches = bf.match(des1, des2)
        good_matches = sorted(good_matches, key=lambda x: x.distance)[:100]  # L·∫•y top 100 matches
    
    # V·∫Ω matched keypoints
    matches_img = cv2.drawMatches(img1, kp1, img2, kp2, good_matches[:20], None, 
                                 flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    
    # ∆Ø·ªõc l∆∞·ª£ng homography n·∫øu c√≥ ƒë·ªß matches
    if len(good_matches) < 10:
        print(f"Warning: Not enough good matches ({len(good_matches)}). Skipping stitching.")
        return img2, matches_img, 0
    
    # ∆Ø·ªõc l∆∞·ª£ng homography
    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
    
    # Th·ª≠ RANSAC v·ªõi nhi·ªÅu iteration h∆°n v√† threshold ph√π h·ª£p h∆°n
    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 3.0, maxIters=10000)
    inliers = int(np.sum(mask))
    
    # Ki·ªÉm tra ch·∫•t l∆∞·ª£ng homography (gi·∫£m ng∆∞·ª°ng xu·ªëng)
    min_inliers = min(30, len(good_matches) // 4)  # ƒêi·ªÅu ch·ªânh ng∆∞·ª°ng d·ª±a v√†o s·ªë matches
    if inliers < min_inliers or H is None:
        print(f"Warning: Low quality homography ({inliers} inliers). Skipping stitching.")
        return img2, matches_img, inliers
    
    # T√≠nh k√≠ch th∆∞·ªõc canvas cho ·∫£nh gh√©p
    h1, w1 = img1.shape[:2]
    h2, w2 = img2.shape[:2]
    
    # T√≠nh to√°n k√≠ch th∆∞·ªõc v√† v·ªã tr√≠ canvas
    corners1 = np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2)
    corners2 = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)
    
    # Bi·∫øn ƒë·ªïi g√≥c c·ªßa ·∫£nh 1 theo homography
    warped_corners = cv2.perspectiveTransform(corners1, H)
    
    # K·∫øt h·ª£p t·∫•t c·∫£ c√°c g√≥c ƒë·ªÉ t√¨m k√≠ch th∆∞·ªõc t·ªëi ƒëa
    all_corners = np.concatenate((warped_corners, corners2), axis=0)
    
    # T√¨m gi·ªõi h·∫°n canvas
    x_min, y_min = np.int32(all_corners.min(axis=0).ravel() - 0.5)
    x_max, y_max = np.int32(all_corners.max(axis=0).ravel() + 0.5)
    
    # ƒê·∫£m b·∫£o k√≠ch th∆∞·ªõc h·ª£p l√Ω
    width = x_max - x_min
    height = y_max - y_min
    
    if width <= 0 or height <= 0 or width > 10000 or height > 10000:
        print(f"Warning: Invalid dimensions ({width}x{height}). Skipping stitching.")
        return img2, matches_img, inliers
    
    # Ma tr·∫≠n d·ªãch chuy·ªÉn ƒë·ªÉ ƒë∆∞a v·ªÅ v√πng d∆∞∆°ng
    translation = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]], dtype=np.float64)
    H_adjusted = translation @ H
    
    # T·∫°o ·∫£nh warp t·ª´ ·∫£nh 1
    warped_img = cv2.warpPerspective(img1, H_adjusted, (width, height))
    
    # T·∫°o mask cho v√πng warp
    mask1 = cv2.warpPerspective(np.ones((h1, w1), dtype=np.uint8), H_adjusted, (width, height))
    
    # T·∫°o canvas cho ·∫£nh k·∫øt qu·∫£
    result = np.zeros((height, width, 3), dtype=np.uint8)
    
    # ƒê·∫∑t ·∫£nh 2 v√†o v·ªã tr√≠ th√≠ch h·ª£p trong canvas
    y_offset = max(0, -y_min)
    x_offset = max(0, -x_min)
    
    # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° k√≠ch th∆∞·ªõc
    y_end = min(height, y_offset + h2)
    x_end = min(width, x_offset + w2)
    y2_end = min(h2, y_end - y_offset)
    x2_end = min(w2, x_end - x_offset)
    
    # Ch√®n ·∫£nh 2 v√†o v·ªã tr√≠ th√≠ch h·ª£p
    result[y_offset:y_offset + y2_end, x_offset:x2_end] = img2[:y2_end, :x2_end]
    
    # T·∫°o mask cho ·∫£nh 2
    mask2 = np.zeros((height, width), dtype=np.uint8)
    mask2[y_offset:y_offset + y2_end, x_offset:x2_end] = 1
    
    # T√≠nh v√πng ch·ªìng l·∫•p
    overlap_mask = mask1 & mask2
    
    # T·∫°o gradient mask cho v√πng ch·ªìng l·∫•p ƒë·ªÉ blend m∆∞·ª£t h∆°n
    if np.sum(overlap_mask) > 0:
        # T·∫°o gradient mask t·ª´ tr√°i sang ph·∫£i trong v√πng ch·ªìng l·∫•p
        y_indices, x_indices = np.where(overlap_mask > 0)
        left_edge = np.min(x_indices)
        right_edge = np.max(x_indices)
        
        # T·∫°o gradient t·ª´ 0 ƒë·∫øn 1 trong v√πng ch·ªìng l·∫•p
        width_overlap = right_edge - left_edge + 1
        if width_overlap > 1:  # Tr√°nh chia cho 0
            for x in range(left_edge, right_edge + 1):
                # T·∫°o gradient t·ª´ 0->1 theo chi·ªÅu t·ª´ tr√°i sang ph·∫£i
                alpha = (x - left_edge) / (width_overlap - 1)
                # √Åp d·ª•ng gradient ch·ªâ trong v√πng ch·ªìng l·∫•p t·∫°i c·ªôt x
                col_overlap = overlap_mask[:, x].astype(bool)
                
                # √Åp d·ª•ng alpha blending v·ªõi gradient
                result[col_overlap, x, :] = (
                    (1 - alpha) * result[col_overlap, x, :] + 
                    alpha * warped_img[col_overlap, x, :]
                ).astype(np.uint8)
        
        # √Åp d·ª•ng warped image ·ªü nh·ªØng v√πng kh√¥ng ch·ªìng l·∫•p v·ªõi ·∫£nh 2
        non_overlap_warped = (mask1 > 0) & (mask2 == 0)
        result[non_overlap_warped] = warped_img[non_overlap_warped]
    else:
        # N·∫øu kh√¥ng c√≥ v√πng ch·ªìng l·∫•p, k·∫øt h·ª£p c·∫£ hai ·∫£nh
        result = np.where(mask2[:, :, np.newaxis] > 0, result, warped_img)
    
    return result, matches_img, inliers

@app.route('/stitch', methods=['POST'])
def stitch_images():
    files = request.files
    if len(files) < 4:
        return jsonify({'error': 'At least four images are required'}), 400
    
    # ƒê·ªçc v√† x·ª≠ l√Ω c√°c ·∫£nh
    images = []
    for key in sorted(files.keys()):
        file = files[key]
        try:
            # ƒê·ªçc ·∫£nh ƒë√∫ng c√°ch ƒë·ªÉ ƒë·∫£m b·∫£o m√†u s·∫Øc ch√≠nh x√°c
            img_pil = Image.open(file)
            img = np.array(img_pil)
            
            # Chuy·ªÉn ƒë·ªïi t·ª´ RGB sang BGR n·∫øu c·∫ßn (cho OpenCV)
            if len(img.shape) == 3 and img.shape[2] == 3:
                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
            elif len(img.shape) == 3 and img.shape[2] == 4:  # X·ª≠ l√Ω ·∫£nh RGBA
                img = cv2.cvtColor(img, cv2.COLOR_RGBA2BGR)
            
            images.append(img)
        except Exception as e:
            return jsonify({'error': f'Failed to process image {key}: {str(e)}'}), 400
    
    # Ph·∫ßn code c√≤n l·∫°i gi·ªØ nguy√™n
    # ƒê·ªçc v√† x·ª≠ l√Ω c√°c ·∫£nh
    images = []
    for key in sorted(files.keys()):
        file = files[key]
        try:
            img = np.array(Image.open(file))
            # X·ª≠ l√Ω ƒë·ªãnh d·∫°ng ·∫£nh...
            images.append(img)
        except Exception as e:
            return jsonify({'error': f'Failed to process image {key}: {str(e)}'}), 400
    
    if len(images) < 4:
        return jsonify({'error': 'At least four valid images are required'}), 400
    
    # T·∫°o ·∫£nh matches gi·ªØa c√°c c·∫∑p ·∫£nh theo y√™u c·∫ßu
    all_matches_imgs = []
    all_inliers = []
    
    # Gh√©p gi·ªØa c√°c c·∫∑p ·∫£nh li√™n ti·∫øp: 0-1, 1-2, 2-3
    for i in range(len(images) - 1):
        # G·ªçi h√†m match gi·ªØa hai ·∫£nh li√™n ti·∫øp
        _, current_matches_img, inliers = stitch_two_images(images[i], images[i+1])
        all_matches_imgs.append(current_matches_img)
        all_inliers.append(inliers)
    
    # Th·ª±c hi·ªán gh√©p panorama t·ª´ t·∫•t c·∫£ c√°c ·∫£nh
    result = images[0]
    try:
        for i in range(1, len(images)):
            temp_result, _, _ = stitch_two_images(result, images[i])
            result = temp_result
    except Exception as e:
        print(f"Error during stitching: {str(e)}")
        if not all_matches_imgs:
            return jsonify({'error': f'Stitching failed: {str(e)}'}), 500
    
    # N·∫øu c√≥ nhi·ªÅu h∆°n 4 ·∫£nh, ch·ªâ l·∫•y 3 c·∫∑p ƒë·∫ßu ti√™n
    if len(all_matches_imgs) > 3:
        all_matches_imgs = all_matches_imgs[:3]
        all_inliers = all_inliers[:3]
    
    # Gh√©p c√°c ·∫£nh matches l·∫°i v·ªõi nhau ƒë·ªÉ hi·ªÉn th·ªã
    combined_matches = None
    if len(all_matches_imgs) > 0:
        # ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc c√°c ·∫£nh matches ƒë·ªÉ c√≥ c√πng chi·ªÅu cao
        target_height = 400
        resized_matches = []
        for img in all_matches_imgs:
            h, w = img.shape[:2]
            aspect = w / h
            new_width = int(target_height * aspect)
            resized = cv2.resize(img, (new_width, target_height))
            resized_matches.append(resized)
        
        # Gh√©p ngang c√°c ·∫£nh matches
        combined_matches = np.hstack(resized_matches)
    
    # Chuy·ªÉn ƒë·ªïi panorama v√† matches t·ª´ BGR sang RGB tr∆∞·ªõc khi tr·∫£ v·ªÅ
    if combined_matches is not None:
        combined_matches = cv2.cvtColor(combined_matches, cv2.COLOR_BGR2RGB)
    
    if result is not None:
        result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)
    
    return jsonify({
        'matches': image_to_base64(combined_matches),
        'panorama': image_to_base64(result),
        'inliers': all_inliers
    })

# Import trained models
class NeRF(nn.Module):
    def __init__(self, D=8, W=128, use_viewdirs=True):
        """
        Simple NeRF model
        """
        super(NeRF, self).__init__()
        self.D = D
        self.W = W
        self.use_viewdirs = use_viewdirs
        
        # Create simple MLP
        self.pts_linears = nn.ModuleList(
            [nn.Linear(3, W)] + [nn.Linear(W, W) for i in range(D-1)])
        
        if use_viewdirs:
            self.views_linears = nn.ModuleList([nn.Linear(W+3, W//2)])
            self.feature_linear = nn.Linear(W, W)
            self.alpha_linear = nn.Linear(W, 1)
            self.rgb_linear = nn.Linear(W//2, 3)
        else:
            self.output_linear = nn.Linear(W, 4)
    
    def forward(self, x):
        """Simple forward pass"""
        input_pts, input_views = torch.split(x, [3, 3], dim=-1)
        h = input_pts
        
        for i, l in enumerate(self.pts_linears):
            h = self.pts_linears[i](h)
            h = nn.functional.relu(h)
            
        if self.use_viewdirs:
            alpha = self.alpha_linear(h)
            feature = self.feature_linear(h)
            h = torch.cat([feature, input_views], -1)
            
            for i, l in enumerate(self.views_linears):
                h = self.views_linears[i](h)
                h = nn.functional.relu(h)
                
            rgb = self.rgb_linear(h)
            outputs = torch.cat([rgb, alpha], -1)
        else:
            outputs = self.output_linear(h)
        
        return outputs
    
    def get_point_cloud(self, num_points=5000):
        """Generate a simple point cloud for visualization"""
        # In a real implementation, this would extract points from the NeRF volume
        # Here we just generate a simple sphere for visualization
        points = []
        colors = []
        
        for i in range(num_points):
            # Generate points on a sphere
            theta = 2 * np.pi * np.random.random()
            phi = np.pi * np.random.random()
            r = 0.8 + 0.2 * np.random.random()
            
            x = r * np.sin(phi) * np.cos(theta)
            y = r * np.sin(phi) * np.sin(theta)
            z = r * np.cos(phi)
            
            points.append([x, y, z])
            
            # Generate colors based on position
            color = [
                0.5 + 0.5 * np.sin(theta),
                0.5 + 0.5 * np.cos(phi),
                0.5 + 0.5 * np.sin(theta + phi)
            ]
            colors.append(color)
        
        return torch.tensor(points), torch.tensor(colors)

class GaussianSplatting3D(nn.Module):
    def __init__(self, num_gaussians=5000):
        """
        Simple Gaussian Splatting model
        """
        super(GaussianSplatting3D, self).__init__()
        self.num_gaussians = num_gaussians
        
        # Initialize gaussian centers, scales, rotations and colors
        self.means = nn.Parameter(torch.randn(num_gaussians, 3))
        self.scales = nn.Parameter(torch.ones(num_gaussians, 3) * 0.1)
        self.colors = nn.Parameter(torch.rand(num_gaussians, 3))
        
    def forward(self, rays):
        """Simple forward pass"""
        # This would render the gaussians along rays
        # For simplicity, we just return the colors
        return self.colors
    
    def get_point_cloud(self):
        """Return the gaussian centers and colors as a point cloud"""
        return self.means, self.colors

def load_trained_models():
    """Load trained models with correct architecture"""
    models = {}
    
    # Load NeRF - th·ª≠ .pth tr∆∞·ªõc, sau ƒë√≥ fallback to .npy
    nerf_path = 'models/nerf_best.pth'
    nerf_points_file = 'models/nerf_points.npy'
    nerf_colors_file = 'models/nerf_colors.npy'
    
    nerf_loaded = False
    
    # Th·ª≠ load .pth file tr∆∞·ªõc
    if os.path.exists(nerf_path):
        try:
            nerf_model = NeRF(D=8, W=128, use_viewdirs=True)
            state_dict = torch.load(nerf_path, map_location=device)
            nerf_model.load_state_dict(state_dict)
            nerf_model.eval()
            nerf_model = nerf_model.to(device)
            models['nerf'] = nerf_model
            print("‚úÖ NeRF model (.pth) loaded successfully!")
            nerf_loaded = True
        except Exception as e:
            print(f"‚ùå Error loading NeRF .pth: {e}")
    
    # N·∫øu .pth fail, load t·ª´ .npy
    if not nerf_loaded and os.path.exists(nerf_points_file) and os.path.exists(nerf_colors_file):
        try:
            points = np.load(nerf_points_file)
            colors = np.load(nerf_colors_file)
            print(f"üìÇ Loading NeRF from .npy: {points.shape} points, {colors.shape} colors")
            
            # T·∫°o fake model ƒë·ªÉ store point cloud  
            fake_model = type('FakeNeRF', (), {
                'points': torch.FloatTensor(points),
                'colors': torch.FloatTensor(colors),
                'get_point_cloud': lambda self: (self.points, self.colors)
            })()
            models['nerf'] = fake_model
            print("‚úÖ NeRF point cloud loaded from .npy files!")
            nerf_loaded = True
        except Exception as e:
            print(f"‚ùå Failed to load NeRF from .npy: {e}")
    
    if not nerf_loaded:
        print("‚ö†Ô∏è NeRF model not loaded")

    # Load Gaussian Splatting t∆∞∆°ng t·ª±
    gs_path = 'models/gaussian_splatting_best.pth'
    gs_points_file = 'models/gaussian_splatting_points.npy'
    gs_colors_file = 'models/gaussian_splatting_colors.npy'
    
    gs_loaded = False
    
    # Th·ª≠ load .pth file tr∆∞·ªõc
    if os.path.exists(gs_path):
        try:
            gs_model = GaussianSplatting3D(num_gaussians=5000)
            state_dict = torch.load(gs_path, map_location=device)
            gs_model.load_state_dict(state_dict)
            gs_model.eval()
            gs_model = gs_model.to(device)
            models['gaussian_splatting'] = gs_model
            print("‚úÖ Gaussian Splatting model (.pth) loaded successfully!")
            gs_loaded = True
        except Exception as e:
            print(f"‚ùå Error loading Gaussian Splatting .pth: {e}")
    
    # N·∫øu .pth fail, load t·ª´ .npy
    if not gs_loaded and os.path.exists(gs_points_file) and os.path.exists(gs_colors_file):
        try:
            points = np.load(gs_points_file)
            colors = np.load(gs_colors_file)
            print(f"üìÇ Loading Gaussian Splatting from .npy: {points.shape} points, {colors.shape} colors")
            
            fake_model = type('FakeGS', (), {
                'points': torch.FloatTensor(points),
                'colors': torch.FloatTensor(colors),
                'get_point_cloud': lambda self: (self.points, self.colors)
            })()
            models['gaussian_splatting'] = fake_model
            print("‚úÖ Gaussian Splatting point cloud loaded from .npy files!")
            gs_loaded = True
        except Exception as e:
            print(f"‚ùå Failed to load Gaussian Splatting from .npy: {e}")
    
    if not gs_loaded:
        print("‚ö†Ô∏è Gaussian Splatting model not loaded")
    
    print(f"üìä Total models loaded: {len(models)}")
    return models

# Load model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
os.makedirs('models', exist_ok=True)

# T·∫°o d·ªØ li·ªáu m·∫´u n·∫øu ch∆∞a c√≥
if not os.path.exists('models/nerf_points.npy'):
    print("üîÑ Creating sample NeRF point cloud...")
    # T·∫°o sphere pattern
    points = []
    colors = []
    for i in range(5000):
        theta = 2 * np.pi * i / 5000
        phi = np.pi * (i % 20) / 20
        r = 0.8 + 0.2 * np.sin(theta * 3)
        
        x = r * np.sin(phi) * np.cos(theta)
        y = r * np.sin(phi) * np.sin(theta) 
        z = r * np.cos(phi)
        
        points.append([x, y, z])
        colors.append([
            0.6 + 0.4 * np.sin(theta),
            0.6 + 0.4 * np.cos(theta),
            0.6 + 0.4 * np.sin(phi)
        ])
    
    np.save('models/nerf_points.npy', np.array(points))
    np.save('models/nerf_colors.npy', np.array(colors))
    print("‚úÖ Sample NeRF point cloud created!")

if not os.path.exists('models/gaussian_splatting_points.npy'):
    print("üîÑ Creating sample Gaussian Splatting point cloud...")
    # T·∫°o torus pattern
    points = []
    colors = []
    for i in range(5000):
        theta = 2 * np.pi * i / 5000
        phi = 2 * np.pi * (i % 50) / 50
        r1 = 1.0  # major radius
        r2 = 0.3  # minor radius
        
        x = (r1 + r2 * np.cos(phi)) * np.cos(theta)
        y = (r1 + r2 * np.cos(phi)) * np.sin(theta)
        z = r2 * np.sin(phi)
        
        points.append([x, y, z])
        colors.append([
            0.5 + 0.5 * np.sin(theta),
            0.5 + 0.5 * np.cos(phi),
            0.5 + 0.5 * np.sin(theta + phi)
        ])
    
    np.save('models/gaussian_splatting_points.npy', np.array(points))
    np.save('models/gaussian_splatting_colors.npy', np.array(colors))
    print("‚úÖ Sample Gaussian Splatting point cloud created!")

# Load trained models
trained_models = load_trained_models()

def generate_point_cloud_from_nerf(model, num_points=5000):
    """Generate point cloud using trained NeRF model - FIXED VERSION"""
    print("üîÑ Generating point cloud from NeRF...")
    
    try:
        # N·∫øu l√† fake model (loaded t·ª´ .npy)
        if hasattr(model, 'points') and hasattr(model, 'colors'):
            points = model.points.cpu().numpy() if hasattr(model.points, 'cpu') else model.points.numpy()
            colors = model.colors.cpu().numpy() if hasattr(model.colors, 'cpu') else model.colors.numpy()
            
            print(f"üìä Loaded NeRF point cloud: {len(points)} points")
            return points, colors
        
        # N·∫øu l√† model th·∫≠t, extract point cloud
        if hasattr(model, 'get_point_cloud'):
            points, colors = model.get_point_cloud()
            points = points.cpu().numpy()
            colors = colors.cpu().numpy()
            print(f"‚úÖ Generated {len(points)} points from NeRF model")
            return points, colors
            
    except Exception as e:
        print(f"‚ùå Error generating from NeRF: {e}")
    
    # Final fallback - t·∫°o point cloud c√≥ √Ω nghƒ©a
    print("‚ö†Ô∏è Using structured fallback points for NeRF")
    points = []
    colors = []
    
    # T·∫°o sphere pattern
    for i in range(num_points):
        theta = 2 * np.pi * i / num_points
        phi = np.pi * (i % 20) / 20
        r = 0.8 + 0.2 * np.sin(theta * 3)
        
        x = r * np.sin(phi) * np.cos(theta)
        y = r * np.sin(phi) * np.sin(theta) 
        z = r * np.cos(phi)
        
        points.append([x, y, z])
        colors.append([
            0.6 + 0.4 * np.sin(theta),
            0.6 + 0.4 * np.cos(theta),
            0.6 + 0.4 * np.sin(phi)
        ])
    
    return np.array(points), np.array(colors)

def generate_point_cloud_from_gaussian_splatting(model, image_size=(256, 256)):
    """Generate point cloud using trained Gaussian Splatting model"""
    print("üîÑ Generating point cloud from Gaussian Splatting...")
    
    try:
        # N·∫øu l√† fake model (loaded t·ª´ .npy)
        if hasattr(model, 'points') and hasattr(model, 'colors'):
            points = model.points.cpu().numpy() if hasattr(model.points, 'cpu') else model.points.numpy()
            colors = model.colors.cpu().numpy() if hasattr(model.colors, 'cpu') else model.colors.numpy()
            
            print(f"üìä Raw GS data shapes - Points: {points.shape}, Colors: {colors.shape}")
            
            # Ensure colors are in [0,1] range
            colors = np.clip(colors, 0, 1)
            
            print(f"‚úÖ Using cached Gaussian Splatting point cloud: {len(points)} points")
            return points, colors
        
        # N·∫øu l√† model th·∫≠t
        if hasattr(model, 'get_point_cloud'):
            points, colors = model.get_point_cloud()
            points = points.cpu().numpy()
            colors = colors.cpu().numpy()
            print(f"‚úÖ Generated {len(points)} points from Gaussian Splatting")
            return points, colors
            
    except Exception as e:
        print(f"‚ùå Error generating from Gaussian Splatting: {e}")
    
    # Final fallback
    print("‚ö†Ô∏è Using fallback points for Gaussian Splatting")
    points = np.random.randn(1000, 3) * 0.8
    colors = np.random.rand(1000, 3)
    return points, colors

def process_images_to_pointcloud(images, model_type='both'):
    """Process uploaded images to generate point cloud using trained models"""
    print(f"üîÑ Processing {len(images)} images with {model_type} model(s)")
    
    results = {}
    
    # Generate point clouds using available models
    if model_type in ['nerf', 'both'] and 'nerf' in trained_models:
        points_nerf, colors_nerf = generate_point_cloud_from_nerf(trained_models['nerf'])
        
        # Minimal logging
        print(f"‚úÖ NeRF: {len(points_nerf)} points generated")
        
        # Convert to format expected by frontend - FLATTEN CORRECTLY
        point_cloud_nerf = points_nerf.flatten().tolist()
        colors_nerf_list = colors_nerf.flatten().tolist()
        
        results['nerf'] = {
            'pointCloud': point_cloud_nerf,
            'numPoints': len(points_nerf),
            'colors': colors_nerf_list
        }
    
    if model_type in ['gaussian_splatting', 'both'] and 'gaussian_splatting' in trained_models:
        points_gs, colors_gs = generate_point_cloud_from_gaussian_splatting(trained_models['gaussian_splatting'])
        
        # Minimal logging
        print(f"‚úÖ GS: {len(points_gs)} points generated")
        
        # Convert to format expected by frontend - FLATTEN CORRECTLY
        point_cloud_gs = points_gs.flatten().tolist()
        colors_gs_list = colors_gs.flatten().tolist()
        
        results['gaussian_splatting'] = {
            'pointCloud': point_cloud_gs,
            'numPoints': len(points_gs),
            'colors': colors_gs_list
        }
    
    print(f"üéØ Completed: {list(results.keys())}")
    return results

def calculate_real_metrics(point_cloud_data):
    """Calculate real metrics from generated point clouds"""
    metrics = {}
    
    for model_name, data in point_cloud_data.items():
        num_points = data['numPoints']
        points = np.array(data['pointCloud']).reshape(-1, 3)
        
        # Calculate some real metrics
        # Point density
        if len(points) > 1:
            # Average distance between points
            from scipy.spatial.distance import pdist
            distances = pdist(points[:min(1000, len(points))])  # Sample for efficiency
            avg_distance = np.mean(distances)
            density = 1.0 / avg_distance if avg_distance > 0 else 0
        else:
            density = 0
        
        # Bounding box volume
        if len(points) > 0:
            bbox_min = np.min(points, axis=0)
            bbox_max = np.max(points, axis=0)
            volume = np.prod(bbox_max - bbox_min)
        else:
            volume = 0
        
        # Coverage (how well points fill the space)
        coverage = min(1.0, density * volume / 1000) if volume > 0 else 0
        
        metrics[model_name] = {
            'num_points': num_points,
            'density': f"{density:.3f}",
            'volume': f"{volume:.3f}",
            'coverage': f"{coverage:.3f}",
            'quality_score': f"{(coverage * 0.6 + (num_points/5000) * 0.4):.3f}"
        }
    
    return metrics

@app.route('/reconstruct', methods=['POST'])
def reconstruct():
    try:
        # Get uploaded images
        images = request.files.getlist('images')
        
        if len(images) == 0:
            return jsonify({'error': 'No images uploaded'}), 400
        
        # Get model type from request (default: both)
        model_type = request.form.get('model_type', 'both')
        
        print(f"üîÑ Received {len(images)} images, using {model_type} model(s)")
        
        # Process images and generate point clouds
        point_cloud_data = process_images_to_pointcloud(images, model_type)
        
        # Calculate real metrics
        metrics = calculate_real_metrics(point_cloud_data)
        
        # Prepare response
        response = {
            'success': True,
            'message': f'Successfully processed {len(images)} images',
            'models_used': list(point_cloud_data.keys()),
            'data': point_cloud_data,
            'metrics': metrics
        }
        
        print(f"‚úÖ Reconstruction complete: {list(point_cloud_data.keys())}")
        return jsonify(response)
        
    except Exception as e:
        print(f"‚ùå Error in reconstruction: {str(e)}")
        return jsonify({
            'error': str(e),
            'success': False
        }), 500

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'available_models': list(trained_models.keys()),
        'nerf_loaded': 'nerf' in trained_models,
        'gaussian_splatting_loaded': 'gaussian_splatting' in trained_models,
        'device': str(device),
        'cuda_available': torch.cuda.is_available()
    })

if __name__ == '__main__':
    app.run(debug=True, port=5000)